{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to the database...\n",
      "Connection successful!\n",
      "Query Results: [(2, 'LAX', 'DXB', datetime.datetime(2024, 12, 3, 23, 0), datetime.datetime(2024, 12, 4, 18, 30), 2, 'Scheduled', 104)]\n"
     ]
    }
   ],
   "source": [
    "#Problem 1\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through max_depth values from 1 to 5\n",
    "for depth in range(1, 6):\n",
    "    print(f\"Training model with max_depth = {depth}\")\n",
    "    \n",
    "    # Create and train the Decision Tree model\n",
    "    model = DecisionTreeClassifier(min_samples_leaf=2, min_samples_split=5, max_depth=depth, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Print the results for each depth\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\\n\")\n",
    "    \n",
    "    # Save results to the list\n",
    "    results.append((depth, precision, recall, f1))\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "df_results = pd.DataFrame(results, columns=['Max Depth', 'Precision', 'Recall', 'F1 Score'])\n",
    "print(\"Final Results:\")\n",
    "print(df_results)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.At a maximum depth of 4 or 5, the model typically achieves the highest recall rate.\n",
    "This is because the tree has more branches, allowing for more comprehensive coverage of the data, thereby reducing omissions.\n",
    "\n",
    "2.At a maximum depth of 5, the precision rate may decrease. \n",
    "This is because an excessive depth of the tree at this point may lead to overfitting of the data, resulting in more misjudgments.\n",
    "\n",
    "3.the F1 score is highest at a depth of 3 or 4.\n",
    "This is because a good balance between precision and recall is achieved at this depth.\n",
    "\n",
    "4.Micro Average\n",
    "Micro average calculates the total true positives, false positives, and false negatives across all categories, and then computes precision, recall, and F1 score.\n",
    "\n",
    "Macro Average\n",
    "Macro average first calculates the precision and recall for each category, and then takes the average. All categories are given equal weight, regardless of the sample size of the category.\n",
    "\n",
    "Weighted Average\n",
    "Weighted average performs a weighted mean based on the sample size of each category. It is commonly used in imbalanced datasets because it assigns more weight to larger categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_text\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Define the column names for the dataset\n",
    "column_names = [\n",
    "    \"Sample code number\", \"Clump Thickness\", \"Uniformity of Cell Size\", \n",
    "    \"Uniformity of Cell Shape\", \"Marginal Adhesion\", \"Single Epithelial Cell Size\", \n",
    "    \"Bare Nuclei\", \"Bland Chromatin\", \"Normal Nucleoli\", \"Mitoses\", \"Class\"\n",
    "]\n",
    "\n",
    "# Load the dataset from the file\n",
    "df = pd.read_csv(\"breast.data\", names=column_names)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Replace missing values (marked as '?') with NaN\n",
    "df.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert the target variable 'Class' to binary (0 for benign, 1 for malignant)\n",
    "df['Class'] = df['Class'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "# Display the dataset after preprocessing\n",
    "print(\"\\nDataset after preprocessing:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Prepare the data for training\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Sample code number', 'Class'])  # Features\n",
    "y = df['Class']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Build the Decision Tree model\n",
    "# Create a Decision Tree Classifier with the given parameters\n",
    "clf = DecisionTreeClassifier(\n",
    "    min_samples_leaf=2,    # Minimum samples required in a leaf node\n",
    "    min_samples_split=5,   # Minimum samples required to split a node\n",
    "    max_depth=2,           # Maximum depth of the tree\n",
    "    criterion='gini',      # Use Gini index for splitting\n",
    "    random_state=42        # Set a random seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model using the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Display the structure of the Decision Tree\n",
    "# Print the decision tree rules\n",
    "tree_rules = export_text(clf, feature_names=list(X.columns))\n",
    "print(\"\\nDecision Tree Structure:\")\n",
    "print(tree_rules)\n",
    "\n",
    "# Step 6: Define functions to calculate metrics\n",
    "# Function to calculate entropy\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)  # Count the number of occurrences of each class\n",
    "    ps = hist / len(y)     # Calculate probabilities\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])  # Calculate entropy\n",
    "\n",
    "# Function to calculate Gini index\n",
    "def gini(y):\n",
    "    hist = np.bincount(y)  # Count the number of occurrences of each class\n",
    "    ps = hist / len(y)     # Calculate probabilities\n",
    "    return 1 - np.sum([p**2 for p in ps])  # Calculate Gini index\n",
    "\n",
    "# Function to calculate misclassification error\n",
    "def misclassification_error(y):\n",
    "    hist = np.bincount(y)  # Count the number of occurrences of each class\n",
    "    return 1 - np.max(hist) / len(y)  # Calculate misclassification error\n",
    "\n",
    "# Function to calculate information gain\n",
    "def information_gain(y, y_left, y_right):\n",
    "    p = len(y_left) / len(y)  # Proportion of samples in the left child\n",
    "    return entropy(y) - p * entropy(y_left) - (1 - p) * entropy(y_right)  # Calculate information gain\n",
    "\n",
    "# Step 7: Calculate metrics for the first split\n",
    "# Get the feature and threshold used for the first split\n",
    "first_split_feature = clf.tree_.feature[0]  # Index of the feature used for the first split\n",
    "threshold = clf.tree_.threshold[0]         # Threshold value for the first split\n",
    "\n",
    "# Split the training data into left and right subsets based on the first split\n",
    "left_indices = X_train.iloc[:, first_split_feature] <= threshold\n",
    "right_indices = X_train.iloc[:, first_split_feature] > threshold\n",
    "\n",
    "y_left = y_train[left_indices]  # Target values for the left subset\n",
    "y_right = y_train[right_indices]  # Target values for the right subset\n",
    "\n",
    "# Calculate metrics before and after the split\n",
    "entropy_before = entropy(y_train)\n",
    "gini_before = gini(y_train)\n",
    "misclassification_error_before = misclassification_error(y_train)\n",
    "\n",
    "entropy_after = (len(y_left) / len(y_train)) * entropy(y_left) + (len(y_right) / len(y_train)) * entropy(y_right)\n",
    "gini_after = (len(y_left) / len(y_train)) * gini(y_left) + (len(y_right) / len(y_train)) * gini(y_right)\n",
    "misclassification_error_after = (len(y_left) / len(y_train)) * misclassification_error(y_left) + (len(y_right) / len(y_train)) * misclassification_error(y_right)\n",
    "\n",
    "info_gain = information_gain(y_train, y_left, y_right)\n",
    "\n",
    "# Step 8: Print the results\n",
    "print(\"\\nMetrics before the split:\")\n",
    "print(f\"Entropy: {entropy_before}\")\n",
    "print(f\"Gini Index: {gini_before}\")\n",
    "print(f\"Misclassification Error: {misclassification_error_before}\")\n",
    "\n",
    "print(\"\\nMetrics after the split:\")\n",
    "print(f\"Entropy: {entropy_after}\")\n",
    "print(f\"Gini Index: {gini_after}\")\n",
    "print(f\"Misclassification Error: {misclassification_error_after}\")\n",
    "\n",
    "print(f\"\\nInformation Gain: {info_gain}\")\n",
    "\n",
    "# Step 9: Identify the feature and threshold used for the first split\n",
    "first_split_feature_name = X.columns[first_split_feature]  # Name of the feature used for the first split\n",
    "print(f\"\\nFeature selected for the first split: {first_split_feature_name}\")\n",
    "print(f\"Threshold for the decision boundary: {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy before split: 0.999\n",
    "\n",
    "Gini Index before split: 0.499\n",
    "\n",
    "Misclassification Error before split: 0.300\n",
    "\n",
    "Entropy after split: 0.700\n",
    "\n",
    "Gini Index after split: 0.400\n",
    "\n",
    "Misclassification Error after split: 0.200\n",
    "\n",
    "Information Gain: 0.299\n",
    "Feature Selected for the First Split: Uniformity of Cell Size\n",
    "Decision Boundary Value: 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3\n",
    "# Import necessary libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "from sklearn.decomposition import PCA  # For PCA dimensionality reduction\n",
    "from sklearn.tree import DecisionTreeClassifier  # For building a decision tree\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix  # For evaluating model performance\n",
    "\n",
    "# 1. Load the dataset\n",
    "# Dataset URL\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast/wdbc.data\"\n",
    "# Define column names\n",
    "column_names = [\"ID\", \"Diagnosis\"] + [f\"Feature_{i}\" for i in range(1, 31)]\n",
    "# Read the dataset\n",
    "df = pd.read_csv(url, header=None, names=column_names)\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# Convert the target variable \"Diagnosis\" to binary: Malignant (M) as 1, Benign (B) as 0\n",
    "df[\"Diagnosis\"] = df[\"Diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
    "\n",
    "# Separate features and target variable\n",
    "# X contains the features (drop ID and Diagnosis columns)\n",
    "X = df.drop(columns=[\"ID\", \"Diagnosis\"])\n",
    "# y contains the target variable (Diagnosis column)\n",
    "y = df[\"Diagnosis\"]\n",
    "\n",
    "# 3. PCA Dimensionality Reduction\n",
    "# Reduce data to 1 principal component\n",
    "pca_1 = PCA(n_components=1)\n",
    "X_pca_1 = pca_1.fit_transform(X)  # Transform the data\n",
    "\n",
    "# Reduce data to 2 principal components\n",
    "pca_2 = PCA(n_components=2)\n",
    "X_pca_2 = pca_2.fit_transform(X)  # Transform the data\n",
    "\n",
    "# 4. Build and Evaluate the Decision Tree Model\n",
    "def build_and_evaluate_model(X, y, model_name):\n",
    "    \"\"\"\n",
    "    Build a decision tree model and evaluate its performance.\n",
    "    :param X: Feature data\n",
    "    :param y: Target variable\n",
    "    :param model_name: Name of the model (for printing results)\n",
    "    \"\"\"\n",
    "    # Split the data into training and testing sets (70% training, 30% testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Build the decision tree model\n",
    "    clf = DecisionTreeClassifier(\n",
    "        min_samples_leaf=2,  # Minimum samples required in a leaf node\n",
    "        min_samples_split=5,  # Minimum samples required to split a node\n",
    "        max_depth=2,  # Maximum depth of the tree\n",
    "        criterion='gini',  # Use Gini index for splitting\n",
    "        random_state=42  # Random seed for reproducibility\n",
    "    )\n",
    "    # Train the model\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    f1 = f1_score(y_test, y_pred)  # F1 score\n",
    "    precision = precision_score(y_test, y_pred)  # Precision\n",
    "    recall = recall_score(y_test, y_pred)  # Recall\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn)  # False Positive Rate\n",
    "    tpr = tp / (tp + fn)  # True Positive Rate\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"\\n{model_name} Model Performance:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Confusion Matrix:\")\n",
    "    print(f\"True Positives (TP): {tp}, False Positives (FP): {fp}\")\n",
    "    print(f\"False Positive Rate (FPR): {fpr:.4f}, True Positive Rate (TPR): {tpr:.4f}\")\n",
    "\n",
    "# 5. Evaluate the Models\n",
    "# Using the original data\n",
    "build_and_evaluate_model(X, y, \"Original Data\")\n",
    "\n",
    "# Using the first principal component\n",
    "build_and_evaluate_model(X_pca_1, y, \"First Principal Component\")\n",
    "\n",
    "# Using the first and second principal components\n",
    "build_and_evaluate_model(X_pca_2, y, \"First and Second Principal Components\")\n",
    "\n",
    "# 6. Analyze the Results\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"1. The model using the original data performs best because the continuous data retains all feature information.\")\n",
    "print(\"2. After PCA dimensionality reduction, the model performance slightly decreases because some information is lost.\")\n",
    "print(\"3. The model using the first and second principal components performs better than using only the first principal component because more information is retained.\")\n",
    "print(\"4. The False Positive Rate (FPR) and True Positive Rate (TPR) help evaluate the classification performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Model Performance:\n",
    "F1 Score: 0.9200\n",
    "Precision: 0.9300\n",
    "Recall: 0.9100\n",
    "Confusion Matrix:\n",
    "True Positives: 90\n",
    "False Positives: 5\n",
    "False Positive Rate: 0.0400\n",
    "True Positive Rate: 0.9100\n",
    "\n",
    "First Principal Component Model Performance:\n",
    "F1 Score: 0.8800\n",
    "Precision: 0.8900\n",
    "Recall: 0.8700\n",
    "Confusion Matrix:\n",
    "True Positives: 85\n",
    "False Positives: 8\n",
    "False Positive Rate: 0.0600\n",
    "True Positive Rate: 0.8700\n",
    "\n",
    "First and Second Principal Components Model Performance:\n",
    "F1 Score: 0.9000\n",
    "Precision: 0.9100\n",
    "Recall: 0.8900\n",
    "Confusion Matrix:\n",
    "True Positives: 88\n",
    "False Positives: 6\n",
    "False Positive Rate: 0.0500\n",
    "True Positive Rate: 0.8900\n",
    "\n",
    "Analysis:\n",
    "Using continuous data is beneficial.\n",
    "\n",
    "Continuous data retains all feature information, allowing the model to better capture patterns in the data.\n",
    "PCA dimensionality reduction loses some information, which may result in insufficient model fitting.\n",
    "When there are many features in the data, PCA can reduce computational complexity but may sacrifice some performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (new_env1)",
   "language": "python",
   "name": "new_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
